{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e463037f",
   "metadata": {
    "papermill": {
     "duration": 0.009482,
     "end_time": "2024-12-11T12:17:22.749456",
     "exception": false,
     "start_time": "2024-12-11T12:17:22.739974",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<p style=\"text-align:center\">\n",
    "\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "\n",
    "    </a>\n",
    "\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d807e170",
   "metadata": {
    "papermill": {
     "duration": 0.007223,
     "end_time": "2024-12-11T12:17:22.764452",
     "exception": false,
     "start_time": "2024-12-11T12:17:22.757229",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Lab: Custom Training Loops in Keras**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7a63df",
   "metadata": {
    "papermill": {
     "duration": 0.007193,
     "end_time": "2024-12-11T12:17:22.778977",
     "exception": false,
     "start_time": "2024-12-11T12:17:22.771784",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Estimated time needed: **30** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9409d2f3",
   "metadata": {
    "papermill": {
     "duration": 0.007158,
     "end_time": "2024-12-11T12:17:22.793461",
     "exception": false,
     "start_time": "2024-12-11T12:17:22.786303",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this lab, you will learn to implement a basic custom training loop in Keras. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc6fa41",
   "metadata": {
    "papermill": {
     "duration": 0.007176,
     "end_time": "2024-12-11T12:17:22.807981",
     "exception": false,
     "start_time": "2024-12-11T12:17:22.800805",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Objectives\n",
    "\n",
    "\n",
    "\n",
    "By the end of this lab, you will: \n",
    "\n",
    "\n",
    "\n",
    "- Set up the environment \n",
    "\n",
    "\n",
    "\n",
    "- Define the neural network model \n",
    "\n",
    "\n",
    "\n",
    "- Define the Loss Function and Optimizer \n",
    "\n",
    "\n",
    "\n",
    "- Implement the custom training loop \n",
    "\n",
    "\n",
    "\n",
    "- Enhance the custom training loop by adding an accuracy metric to monitor model performance \n",
    "\n",
    "\n",
    "\n",
    "- Implement a custom callback to log additional metrics and information during training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662ea4fd",
   "metadata": {
    "papermill": {
     "duration": 0.007146,
     "end_time": "2024-12-11T12:17:22.822424",
     "exception": false,
     "start_time": "2024-12-11T12:17:22.815278",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12510f87",
   "metadata": {
    "papermill": {
     "duration": 0.007178,
     "end_time": "2024-12-11T12:17:22.836915",
     "exception": false,
     "start_time": "2024-12-11T12:17:22.829737",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step-by-Step Instructions:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acde8932",
   "metadata": {
    "papermill": {
     "duration": 0.007506,
     "end_time": "2024-12-11T12:17:22.851715",
     "exception": false,
     "start_time": "2024-12-11T12:17:22.844209",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Exercise 1: Basic custom training loop: \n",
    "\n",
    "\n",
    "\n",
    "#### 1. Set Up the Environment:\n",
    "\n",
    "\n",
    "\n",
    "- Import necessary libraries. \n",
    "\n",
    "\n",
    "\n",
    "- Load and preprocess the MNIST dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb3fa8ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T12:17:22.867331Z",
     "iopub.status.busy": "2024-12-11T12:17:22.867072Z",
     "iopub.status.idle": "2024-12-11T12:17:22.871357Z",
     "shell.execute_reply": "2024-12-11T12:17:22.870614Z"
    },
    "papermill": {
     "duration": 0.013868,
     "end_time": "2024-12-11T12:17:22.872873",
     "exception": false,
     "start_time": "2024-12-11T12:17:22.859005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "216874ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T12:17:22.888622Z",
     "iopub.status.busy": "2024-12-11T12:17:22.888376Z",
     "iopub.status.idle": "2024-12-11T12:17:36.910481Z",
     "shell.execute_reply": "2024-12-11T12:17:36.909525Z"
    },
    "papermill": {
     "duration": 14.03232,
     "end_time": "2024-12-11T12:17:36.912595",
     "exception": false,
     "start_time": "2024-12-11T12:17:22.880275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import warnings\n",
    "\n",
    "import tensorflow as tf \n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Suppress all Python warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "# Set TensorFlow log level to suppress warnings and info messages\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Set Up the Environment\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() \n",
    "\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 \n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3446896d",
   "metadata": {
    "papermill": {
     "duration": 0.007591,
     "end_time": "2024-12-11T12:17:36.928500",
     "exception": false,
     "start_time": "2024-12-11T12:17:36.920909",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 2. Define the model: \n",
    "\n",
    "\n",
    "\n",
    "Create a simple neural network model with a Flatten layer followed by two Dense layers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94085948",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T12:17:36.945228Z",
     "iopub.status.busy": "2024-12-11T12:17:36.944707Z",
     "iopub.status.idle": "2024-12-11T12:17:37.008328Z",
     "shell.execute_reply": "2024-12-11T12:17:37.007475Z"
    },
    "papermill": {
     "duration": 0.073982,
     "end_time": "2024-12-11T12:17:37.010111",
     "exception": false,
     "start_time": "2024-12-11T12:17:36.936129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 2: Define the Model\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "\n",
    "    Dense(128, activation='relu'),\n",
    "\n",
    "    Dense(10)\n",
    "\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0b6d67",
   "metadata": {
    "papermill": {
     "duration": 0.007575,
     "end_time": "2024-12-11T12:17:37.025625",
     "exception": false,
     "start_time": "2024-12-11T12:17:37.018050",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 3. Define Loss Function and Optimizer: \n",
    "\n",
    "\n",
    "\n",
    "- Use Sparse Categorical Crossentropy for the loss function. \n",
    "\n",
    "- Use the Adam optimizer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "816dff60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T12:17:37.042368Z",
     "iopub.status.busy": "2024-12-11T12:17:37.042117Z",
     "iopub.status.idle": "2024-12-11T12:17:37.050231Z",
     "shell.execute_reply": "2024-12-11T12:17:37.049659Z"
    },
    "papermill": {
     "duration": 0.018284,
     "end_time": "2024-12-11T12:17:37.051886",
     "exception": false,
     "start_time": "2024-12-11T12:17:37.033602",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 3: Define Loss Function and Optimizer\n",
    "\n",
    "\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) \n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2267c225",
   "metadata": {
    "papermill": {
     "duration": 0.007502,
     "end_time": "2024-12-11T12:17:37.067324",
     "exception": false,
     "start_time": "2024-12-11T12:17:37.059822",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 4. Implement the Custom Training Loop: \n",
    "\n",
    "\n",
    "\n",
    "- Iterate over the dataset for a specified number of epochs. \n",
    "\n",
    "- Compute the loss and apply gradients to update the model's weights. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94d950c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T12:17:37.083911Z",
     "iopub.status.busy": "2024-12-11T12:17:37.083689Z",
     "iopub.status.idle": "2024-12-11T12:18:58.951188Z",
     "shell.execute_reply": "2024-12-11T12:18:58.950463Z"
    },
    "papermill": {
     "duration": 81.878165,
     "end_time": "2024-12-11T12:18:58.953168",
     "exception": false,
     "start_time": "2024-12-11T12:17:37.075003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 1\n",
      "Epoch 1 Step 0: Loss = 2.422433614730835\n",
      "Epoch 1 Step 200: Loss = 0.43094944953918457\n",
      "Epoch 1 Step 400: Loss = 0.1787673383951187\n",
      "Epoch 1 Step 600: Loss = 0.17081455886363983\n",
      "Epoch 1 Step 800: Loss = 0.1301220953464508\n",
      "Epoch 1 Step 1000: Loss = 0.3905356526374817\n",
      "Epoch 1 Step 1200: Loss = 0.1962904930114746\n",
      "Epoch 1 Step 1400: Loss = 0.2361823320388794\n",
      "Epoch 1 Step 1600: Loss = 0.20670533180236816\n",
      "Epoch 1 Step 1800: Loss = 0.177461177110672\n",
      "Start of epoch 2\n",
      "Epoch 2 Step 0: Loss = 0.09719976037740707\n",
      "Epoch 2 Step 200: Loss = 0.1787734031677246\n",
      "Epoch 2 Step 400: Loss = 0.07523341476917267\n",
      "Epoch 2 Step 600: Loss = 0.05839746445417404\n",
      "Epoch 2 Step 800: Loss = 0.07521195709705353\n",
      "Epoch 2 Step 1000: Loss = 0.2161884903907776\n",
      "Epoch 2 Step 1200: Loss = 0.11334450542926788\n",
      "Epoch 2 Step 1400: Loss = 0.11065569519996643\n",
      "Epoch 2 Step 1600: Loss = 0.1422249674797058\n",
      "Epoch 2 Step 1800: Loss = 0.09257857501506805\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Implement the Custom Training Loop\n",
    "\n",
    "\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "# train_dataset = train_dataset.repeat(epochs)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    print(f'Start of epoch {epoch + 1}')\n",
    "\n",
    "\n",
    "\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            logits = model(x_batch_train, training=True)  # Forward pass\n",
    "\n",
    "            loss_value = loss_fn(y_batch_train, logits)  # Compute loss\n",
    "\n",
    "\n",
    "\n",
    "        # Compute gradients and update weights\n",
    "\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "\n",
    "\n",
    "        # Logging the loss every 200 steps\n",
    "\n",
    "        if step % 200 == 0:\n",
    "\n",
    "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd4ba0e",
   "metadata": {
    "papermill": {
     "duration": 0.008524,
     "end_time": "2024-12-11T12:18:58.970893",
     "exception": false,
     "start_time": "2024-12-11T12:18:58.962369",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Exercise 2: Adding Accuracy Metric:\n",
    "\n",
    "\n",
    "\n",
    "Enhance the custom training loop by adding an accuracy metric to monitor model performance. \n",
    "\n",
    "\n",
    "\n",
    "#### 1. Set Up the Environment: \n",
    "\n",
    "\n",
    "\n",
    "Follow the setup from Exercise 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f67f412",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T12:18:58.989634Z",
     "iopub.status.busy": "2024-12-11T12:18:58.989347Z",
     "iopub.status.idle": "2024-12-11T12:19:00.307301Z",
     "shell.execute_reply": "2024-12-11T12:19:00.306357Z"
    },
    "papermill": {
     "duration": 1.329591,
     "end_time": "2024-12-11T12:19:00.309401",
     "exception": false,
     "start_time": "2024-12-11T12:18:58.979810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "from tensorflow.keras.models import Sequential \n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten \n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Set Up the Environment\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "\n",
    "\n",
    "# Normalize the pixel values to be between 0 and 1\n",
    "\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 \n",
    "\n",
    "\n",
    "\n",
    "# Create a batched dataset for training\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff39c418",
   "metadata": {
    "papermill": {
     "duration": 0.008681,
     "end_time": "2024-12-11T12:19:00.327451",
     "exception": false,
     "start_time": "2024-12-11T12:19:00.318770",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 2. Define the Model: \n",
    "\n",
    "Use the same model as in Exercise 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2ac2fff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T12:19:00.346633Z",
     "iopub.status.busy": "2024-12-11T12:19:00.345891Z",
     "iopub.status.idle": "2024-12-11T12:19:00.365348Z",
     "shell.execute_reply": "2024-12-11T12:19:00.364538Z"
    },
    "papermill": {
     "duration": 0.030611,
     "end_time": "2024-12-11T12:19:00.366867",
     "exception": false,
     "start_time": "2024-12-11T12:19:00.336256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 2: Define the Model\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential([ \n",
    "\n",
    "    Flatten(input_shape=(28, 28)),  # Flatten the input to a 1D vector\n",
    "\n",
    "    Dense(128, activation='relu'),  # First hidden layer with 128 neurons and ReLU activation\n",
    "\n",
    "    Dense(10)  # Output layer with 10 neurons for the 10 classes (digits 0-9)\n",
    "\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b1f8af",
   "metadata": {
    "papermill": {
     "duration": 0.008459,
     "end_time": "2024-12-11T12:19:00.384008",
     "exception": false,
     "start_time": "2024-12-11T12:19:00.375549",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 3. Define the loss function, optimizer, and metric: \n",
    "\n",
    "\n",
    "\n",
    "- Use Sparse Categorical Crossentropy for the loss function and Adam optimizer. \n",
    "\n",
    "\n",
    "\n",
    "- Add Sparse Categorical Accuracy as a metric. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab126130",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T12:19:00.402594Z",
     "iopub.status.busy": "2024-12-11T12:19:00.402300Z",
     "iopub.status.idle": "2024-12-11T12:19:00.410332Z",
     "shell.execute_reply": "2024-12-11T12:19:00.409776Z"
    },
    "papermill": {
     "duration": 0.019281,
     "end_time": "2024-12-11T12:19:00.411960",
     "exception": false,
     "start_time": "2024-12-11T12:19:00.392679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 3: Define Loss Function, Optimizer, and Metric\n",
    "\n",
    "\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Loss function for multi-class classification\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()  # Adam optimizer for efficient training\n",
    "\n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()  # Metric to track accuracy during training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4fd024",
   "metadata": {
    "papermill": {
     "duration": 0.008915,
     "end_time": "2024-12-11T12:19:00.430043",
     "exception": false,
     "start_time": "2024-12-11T12:19:00.421128",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 4. Implement the custom training loop with accuracy: \n",
    "\n",
    "\n",
    "\n",
    "Track the accuracy during training and print it at regular intervals. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7abef3c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T12:19:00.448728Z",
     "iopub.status.busy": "2024-12-11T12:19:00.448465Z",
     "iopub.status.idle": "2024-12-11T12:22:35.208966Z",
     "shell.execute_reply": "2024-12-11T12:22:35.208258Z"
    },
    "papermill": {
     "duration": 214.771961,
     "end_time": "2024-12-11T12:22:35.210915",
     "exception": false,
     "start_time": "2024-12-11T12:19:00.438954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 1\n",
      "Epoch 1 Step 0: Loss = 2.362532138824463 Accuracy = 0.03125\n",
      "Epoch 1 Step 200: Loss = 0.36684948205947876 Accuracy = 0.8296020030975342\n",
      "Epoch 1 Step 400: Loss = 0.18811894953250885 Accuracy = 0.8662717938423157\n",
      "Epoch 1 Step 600: Loss = 0.16857564449310303 Accuracy = 0.8831114768981934\n",
      "Epoch 1 Step 800: Loss = 0.15807726979255676 Accuracy = 0.8956382870674133\n",
      "Epoch 1 Step 1000: Loss = 0.4261488914489746 Accuracy = 0.9028783440589905\n",
      "Epoch 1 Step 1200: Loss = 0.15987013280391693 Accuracy = 0.9096326231956482\n",
      "Epoch 1 Step 1400: Loss = 0.2672325074672699 Accuracy = 0.91485995054245\n",
      "Epoch 1 Step 1600: Loss = 0.21595686674118042 Accuracy = 0.9181956648826599\n",
      "Epoch 1 Step 1800: Loss = 0.15964923799037933 Accuracy = 0.9221439361572266\n",
      "Start of epoch 2\n",
      "Epoch 2 Step 0: Loss = 0.09971933811903 Accuracy = 0.96875\n",
      "Epoch 2 Step 200: Loss = 0.10087888687849045 Accuracy = 0.9625310897827148\n",
      "Epoch 2 Step 400: Loss = 0.09598030149936676 Accuracy = 0.9593204259872437\n",
      "Epoch 2 Step 600: Loss = 0.056569620966911316 Accuracy = 0.9612104892730713\n",
      "Epoch 2 Step 800: Loss = 0.10676500201225281 Accuracy = 0.9619616270065308\n",
      "Epoch 2 Step 1000: Loss = 0.26615309715270996 Accuracy = 0.9626935720443726\n",
      "Epoch 2 Step 1200: Loss = 0.08885187655687332 Accuracy = 0.9634939432144165\n",
      "Epoch 2 Step 1400: Loss = 0.14562682807445526 Accuracy = 0.9641327857971191\n",
      "Epoch 2 Step 1600: Loss = 0.18654340505599976 Accuracy = 0.9640849232673645\n",
      "Epoch 2 Step 1800: Loss = 0.11047464609146118 Accuracy = 0.9649326801300049\n",
      "Start of epoch 3\n",
      "Epoch 3 Step 0: Loss = 0.0610588900744915 Accuracy = 1.0\n",
      "Epoch 3 Step 200: Loss = 0.050787586718797684 Accuracy = 0.9752798676490784\n",
      "Epoch 3 Step 400: Loss = 0.07407796382904053 Accuracy = 0.9752181768417358\n",
      "Epoch 3 Step 600: Loss = 0.031302765011787415 Accuracy = 0.9755615592002869\n",
      "Epoch 3 Step 800: Loss = 0.061393555253744125 Accuracy = 0.9755774140357971\n",
      "Epoch 3 Step 1000: Loss = 0.12221551686525345 Accuracy = 0.9757117629051208\n",
      "Epoch 3 Step 1200: Loss = 0.061834223568439484 Accuracy = 0.9759835600852966\n",
      "Epoch 3 Step 1400: Loss = 0.08050716668367386 Accuracy = 0.9762000441551208\n",
      "Epoch 3 Step 1600: Loss = 0.0957527905702591 Accuracy = 0.9761867523193359\n",
      "Epoch 3 Step 1800: Loss = 0.0903978943824768 Accuracy = 0.9765408039093018\n",
      "Start of epoch 4\n",
      "Epoch 4 Step 0: Loss = 0.04163679480552673 Accuracy = 1.0\n",
      "Epoch 4 Step 200: Loss = 0.04337178170681 Accuracy = 0.9836753606796265\n",
      "Epoch 4 Step 400: Loss = 0.060648974031209946 Accuracy = 0.9830112457275391\n",
      "Epoch 4 Step 600: Loss = 0.0393449142575264 Accuracy = 0.9831010699272156\n",
      "Epoch 4 Step 800: Loss = 0.06319054961204529 Accuracy = 0.9833411574363708\n",
      "Epoch 4 Step 1000: Loss = 0.08062363415956497 Accuracy = 0.9835164546966553\n",
      "Epoch 4 Step 1200: Loss = 0.05461077392101288 Accuracy = 0.9832951426506042\n",
      "Epoch 4 Step 1400: Loss = 0.038336481899023056 Accuracy = 0.9832485914230347\n",
      "Epoch 4 Step 1600: Loss = 0.0460805743932724 Accuracy = 0.98305743932724\n",
      "Epoch 4 Step 1800: Loss = 0.06574898958206177 Accuracy = 0.9832905530929565\n",
      "Start of epoch 5\n",
      "Epoch 5 Step 0: Loss = 0.04723324254155159 Accuracy = 0.96875\n",
      "Epoch 5 Step 200: Loss = 0.027813348919153214 Accuracy = 0.9880285859107971\n",
      "Epoch 5 Step 400: Loss = 0.05295778065919876 Accuracy = 0.9883104562759399\n",
      "Epoch 5 Step 600: Loss = 0.036584243178367615 Accuracy = 0.9881967306137085\n",
      "Epoch 5 Step 800: Loss = 0.04116807132959366 Accuracy = 0.9881008267402649\n",
      "Epoch 5 Step 1000: Loss = 0.07801976799964905 Accuracy = 0.9883241653442383\n",
      "Epoch 5 Step 1200: Loss = 0.02409306727349758 Accuracy = 0.9878746867179871\n",
      "Epoch 5 Step 1400: Loss = 0.020914597436785698 Accuracy = 0.987665057182312\n",
      "Epoch 5 Step 1600: Loss = 0.03867144137620926 Accuracy = 0.9874882698059082\n",
      "Epoch 5 Step 1800: Loss = 0.045668378472328186 Accuracy = 0.9877498745918274\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Implement the Custom Training Loop with Accuracy\n",
    "\n",
    "\n",
    "\n",
    "epochs = 5  # Number of epochs for training\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    print(f'Start of epoch {epoch + 1}')\n",
    "\n",
    "    \n",
    "\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # Forward pass: Compute predictions\n",
    "\n",
    "            logits = model(x_batch_train, training=True)\n",
    "\n",
    "            # Compute loss\n",
    "\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "\n",
    "        \n",
    "\n",
    "        # Compute gradients\n",
    "\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "\n",
    "        # Apply gradients to update model weights\n",
    "\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        \n",
    "\n",
    "        # Update the accuracy metric\n",
    "\n",
    "        accuracy_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "\n",
    "\n",
    "        # Log the loss and accuracy every 200 steps\n",
    "\n",
    "        if step % 200 == 0:\n",
    "\n",
    "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()} Accuracy = {accuracy_metric.result().numpy()}')\n",
    "\n",
    "    \n",
    "\n",
    "    # Reset the metric at the end of each epoch\n",
    "\n",
    "    accuracy_metric.reset_state()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88355126",
   "metadata": {
    "papermill": {
     "duration": 0.010799,
     "end_time": "2024-12-11T12:22:35.233534",
     "exception": false,
     "start_time": "2024-12-11T12:22:35.222735",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Exercise 3: Custom Callback for Advanced Logging: \n",
    "\n",
    "\n",
    "\n",
    "Implement a custom callback to log additional metrics and information during training. \n",
    "\n",
    "\n",
    "\n",
    "#### 1. Set Up the Environment: \n",
    "\n",
    "\n",
    "\n",
    "Follow the setup from Exercise 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83d12948",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T12:22:35.256278Z",
     "iopub.status.busy": "2024-12-11T12:22:35.255995Z",
     "iopub.status.idle": "2024-12-11T12:22:36.572417Z",
     "shell.execute_reply": "2024-12-11T12:22:36.571749Z"
    },
    "papermill": {
     "duration": 1.330165,
     "end_time": "2024-12-11T12:22:36.574474",
     "exception": false,
     "start_time": "2024-12-11T12:22:35.244309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "from tensorflow.keras.models import Sequential \n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten \n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Set Up the Environment\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "\n",
    "\n",
    "# Normalize the pixel values to be between 0 and 1\n",
    "\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 \n",
    "\n",
    "\n",
    "\n",
    "# Create a batched dataset for training\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f7a142",
   "metadata": {
    "papermill": {
     "duration": 0.010711,
     "end_time": "2024-12-11T12:22:36.596736",
     "exception": false,
     "start_time": "2024-12-11T12:22:36.586025",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 2. Define the Model: \n",
    "\n",
    "\n",
    "\n",
    "Use the same model as in Exercise 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8fe0d79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T12:22:36.619643Z",
     "iopub.status.busy": "2024-12-11T12:22:36.619332Z",
     "iopub.status.idle": "2024-12-11T12:22:36.637837Z",
     "shell.execute_reply": "2024-12-11T12:22:36.637232Z"
    },
    "papermill": {
     "duration": 0.03184,
     "end_time": "2024-12-11T12:22:36.639401",
     "exception": false,
     "start_time": "2024-12-11T12:22:36.607561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 2: Define the Model\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "\n",
    "    Flatten(input_shape=(28, 28)),  # Flatten the input to a 1D vector\n",
    "\n",
    "    Dense(128, activation='relu'),  # First hidden layer with 128 neurons and ReLU activation\n",
    "\n",
    "    Dense(10)  # Output layer with 10 neurons for the 10 classes (digits 0-9)\n",
    "\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607ff15b",
   "metadata": {
    "papermill": {
     "duration": 0.01081,
     "end_time": "2024-12-11T12:22:36.661018",
     "exception": false,
     "start_time": "2024-12-11T12:22:36.650208",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 3. Define Loss Function, Optimizer, and Metric: \n",
    "\n",
    "\n",
    "\n",
    "- Use Sparse Categorical Crossentropy for the loss function and Adam optimizer. \n",
    "\n",
    "\n",
    "\n",
    "- Add Sparse Categorical Accuracy as a metric. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c5f2a32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T12:22:36.683967Z",
     "iopub.status.busy": "2024-12-11T12:22:36.683709Z",
     "iopub.status.idle": "2024-12-11T12:22:36.690564Z",
     "shell.execute_reply": "2024-12-11T12:22:36.689979Z"
    },
    "papermill": {
     "duration": 0.020112,
     "end_time": "2024-12-11T12:22:36.692041",
     "exception": false,
     "start_time": "2024-12-11T12:22:36.671929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 3: Define Loss Function, Optimizer, and Metric\n",
    "\n",
    "\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Loss function for multi-class classification\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()  # Adam optimizer for efficient training\n",
    "\n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()  # Metric to track accuracy during training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8121ed2d",
   "metadata": {
    "papermill": {
     "duration": 0.010816,
     "end_time": "2024-12-11T12:22:36.713733",
     "exception": false,
     "start_time": "2024-12-11T12:22:36.702917",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 4. Implement the custom training loop with custom callback: \n",
    "\n",
    "\n",
    "\n",
    "Create a custom callback to log additional metrics at the end of each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27cebc80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T12:22:36.736605Z",
     "iopub.status.busy": "2024-12-11T12:22:36.736333Z",
     "iopub.status.idle": "2024-12-11T12:22:36.740339Z",
     "shell.execute_reply": "2024-12-11T12:22:36.739649Z"
    },
    "papermill": {
     "duration": 0.017163,
     "end_time": "2024-12-11T12:22:36.741859",
     "exception": false,
     "start_time": "2024-12-11T12:22:36.724696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "\n",
    "\n",
    "# Step 4: Implement the Custom Callback \n",
    "\n",
    "class CustomCallback(Callback):\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "\n",
    "        logs = logs or {}\n",
    "\n",
    "        print(f'End of epoch {epoch + 1}, loss: {logs.get(\"loss\")}, accuracy: {logs.get(\"accuracy\")}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54450e0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T12:22:36.764644Z",
     "iopub.status.busy": "2024-12-11T12:22:36.764391Z",
     "iopub.status.idle": "2024-12-11T12:24:02.983648Z",
     "shell.execute_reply": "2024-12-11T12:24:02.982651Z"
    },
    "papermill": {
     "duration": 86.232565,
     "end_time": "2024-12-11T12:24:02.985335",
     "exception": false,
     "start_time": "2024-12-11T12:22:36.752770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 1\n",
      "Epoch 1 Step 0: Loss = 2.377516031265259 Accuracy = 0.09375\n",
      "Epoch 1 Step 200: Loss = 0.44414642453193665 Accuracy = 0.8325559496879578\n",
      "Epoch 1 Step 400: Loss = 0.18636739253997803 Accuracy = 0.8668173551559448\n",
      "Epoch 1 Step 600: Loss = 0.16016122698783875 Accuracy = 0.8830595016479492\n",
      "Epoch 1 Step 800: Loss = 0.13501176238059998 Accuracy = 0.8961844444274902\n",
      "Epoch 1 Step 1000: Loss = 0.4211769104003906 Accuracy = 0.903190553188324\n",
      "Epoch 1 Step 1200: Loss = 0.1519915610551834 Accuracy = 0.9097106456756592\n",
      "Epoch 1 Step 1400: Loss = 0.20321957767009735 Accuracy = 0.9149268269538879\n",
      "Epoch 1 Step 1600: Loss = 0.23266978561878204 Accuracy = 0.9182541966438293\n",
      "Epoch 1 Step 1800: Loss = 0.18826685845851898 Accuracy = 0.9224909543991089\n",
      "End of epoch 1, loss: 0.03785301372408867, accuracy: 0.9244499802589417\n",
      "Start of epoch 2\n",
      "Epoch 2 Step 0: Loss = 0.10197167098522186 Accuracy = 0.96875\n",
      "Epoch 2 Step 200: Loss = 0.1321263164281845 Accuracy = 0.9600435495376587\n",
      "Epoch 2 Step 400: Loss = 0.11813893914222717 Accuracy = 0.9576059579849243\n",
      "Epoch 2 Step 600: Loss = 0.04080723226070404 Accuracy = 0.9599105715751648\n",
      "Epoch 2 Step 800: Loss = 0.061808474361896515 Accuracy = 0.961454451084137\n",
      "Epoch 2 Step 1000: Loss = 0.22237397730350494 Accuracy = 0.9622252583503723\n",
      "Epoch 2 Step 1200: Loss = 0.07244450598955154 Accuracy = 0.9630516171455383\n",
      "Epoch 2 Step 1400: Loss = 0.11638575792312622 Accuracy = 0.9638651013374329\n",
      "Epoch 2 Step 1600: Loss = 0.1827145516872406 Accuracy = 0.964065432548523\n",
      "Epoch 2 Step 1800: Loss = 0.1660773754119873 Accuracy = 0.964793860912323\n",
      "End of epoch 2, loss: 0.050441063940525055, accuracy: 0.965399980545044\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Implement the Custom Training Loop with Custom Callback\n",
    "\n",
    "\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "custom_callback = CustomCallback()  # Initialize the custom callback\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    print(f'Start of epoch {epoch + 1}')\n",
    "\n",
    "    \n",
    "\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # Forward pass: Compute predictions\n",
    "\n",
    "            logits = model(x_batch_train, training=True)\n",
    "\n",
    "            # Compute loss\n",
    "\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "\n",
    "        \n",
    "\n",
    "        # Compute gradients\n",
    "\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "\n",
    "        # Apply gradients to update model weights\n",
    "\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        \n",
    "\n",
    "        # Update the accuracy metric\n",
    "\n",
    "        accuracy_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "\n",
    "\n",
    "        # Log the loss and accuracy every 200 steps\n",
    "\n",
    "        if step % 200 == 0:\n",
    "\n",
    "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()} Accuracy = {accuracy_metric.result().numpy()}')\n",
    "\n",
    "    \n",
    "\n",
    "    # Call the custom callback at the end of each epoch\n",
    "\n",
    "    custom_callback.on_epoch_end(epoch, logs={'loss': loss_value.numpy(), 'accuracy': accuracy_metric.result().numpy()})\n",
    "\n",
    "    \n",
    "\n",
    "    # Reset the metric at the end of each epoch\n",
    "\n",
    "    accuracy_metric.reset_state()  # Use reset_state() instead of reset_states()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83fef6e",
   "metadata": {
    "papermill": {
     "duration": 0.011782,
     "end_time": "2024-12-11T12:24:03.009534",
     "exception": false,
     "start_time": "2024-12-11T12:24:02.997752",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Exercise 4: Add Hidden Layers \n",
    "\n",
    "\n",
    "\n",
    "Next, you will add a couple of hidden layers to your model. Hidden layers help the model learn complex patterns in the data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab89f13f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T12:24:03.034523Z",
     "iopub.status.busy": "2024-12-11T12:24:03.034257Z",
     "iopub.status.idle": "2024-12-11T12:24:03.052691Z",
     "shell.execute_reply": "2024-12-11T12:24:03.051897Z"
    },
    "papermill": {
     "duration": 0.032905,
     "end_time": "2024-12-11T12:24:03.054277",
     "exception": false,
     "start_time": "2024-12-11T12:24:03.021372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "\n",
    "\n",
    "# Define the input layer\n",
    "\n",
    "input_layer = Input(shape=(28, 28))  # Input layer with shape (28, 28)\n",
    "\n",
    "\n",
    "\n",
    "# Define hidden layers\n",
    "\n",
    "hidden_layer1 = Dense(64, activation='relu')(input_layer)  # First hidden layer with 64 neurons and ReLU activation\n",
    "\n",
    "hidden_layer2 = Dense(64, activation='relu')(hidden_layer1)  # Second hidden layer with 64 neurons and ReLU activation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e7f3c3",
   "metadata": {
    "papermill": {
     "duration": 0.011659,
     "end_time": "2024-12-11T12:24:03.077900",
     "exception": false,
     "start_time": "2024-12-11T12:24:03.066241",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In the above code: \n",
    "\n",
    "\n",
    "\n",
    "`Dense(64, activation='relu')` creates a dense (fully connected) layer with 64 units and ReLU activation function. \n",
    "\n",
    "\n",
    "\n",
    "Each hidden layer takes the output of the previous layer as its input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac12d17",
   "metadata": {
    "papermill": {
     "duration": 0.011633,
     "end_time": "2024-12-11T12:24:03.101373",
     "exception": false,
     "start_time": "2024-12-11T12:24:03.089740",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Exercise 5: Define the output layer \n",
    "\n",
    "\n",
    "\n",
    "Finally, you will define the output layer. Suppose you are working on a binary classification problem, so the output layer will have one unit with a sigmoid activation function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "610debe9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T12:24:03.126212Z",
     "iopub.status.busy": "2024-12-11T12:24:03.125964Z",
     "iopub.status.idle": "2024-12-11T12:24:03.136225Z",
     "shell.execute_reply": "2024-12-11T12:24:03.135534Z"
    },
    "papermill": {
     "duration": 0.024726,
     "end_time": "2024-12-11T12:24:03.137968",
     "exception": false,
     "start_time": "2024-12-11T12:24:03.113242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_layer = Dense(1, activation='sigmoid')(hidden_layer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b288dc43",
   "metadata": {
    "papermill": {
     "duration": 0.011739,
     "end_time": "2024-12-11T12:24:03.161770",
     "exception": false,
     "start_time": "2024-12-11T12:24:03.150031",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In the above code: \n",
    "\n",
    "\n",
    "\n",
    "`Dense(1, activation='sigmoid')` creates a dense layer with 1 unit and a sigmoid activation function, suitable for binary classification. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8ee89d",
   "metadata": {
    "papermill": {
     "duration": 0.011788,
     "end_time": "2024-12-11T12:24:03.185478",
     "exception": false,
     "start_time": "2024-12-11T12:24:03.173690",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Exercise 6: Create the Model \n",
    "\n",
    "\n",
    "\n",
    "Now, you will create the model by specifying the input and output layers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4726204",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T12:24:03.210864Z",
     "iopub.status.busy": "2024-12-11T12:24:03.210619Z",
     "iopub.status.idle": "2024-12-11T12:24:03.215987Z",
     "shell.execute_reply": "2024-12-11T12:24:03.215335Z"
    },
    "papermill": {
     "duration": 0.020156,
     "end_time": "2024-12-11T12:24:03.217836",
     "exception": false,
     "start_time": "2024-12-11T12:24:03.197680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4470a55f",
   "metadata": {
    "papermill": {
     "duration": 0.011862,
     "end_time": "2024-12-11T12:24:03.241917",
     "exception": false,
     "start_time": "2024-12-11T12:24:03.230055",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In the above code: \n",
    "\n",
    "\n",
    "\n",
    "`Model(inputs=input_layer, outputs=output_layer)` creates a Keras model that connects the input layer to the output layer through the hidden layers. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ba194b",
   "metadata": {
    "papermill": {
     "duration": 0.011844,
     "end_time": "2024-12-11T12:24:03.265734",
     "exception": false,
     "start_time": "2024-12-11T12:24:03.253890",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Exercise 7: Compile the Model \n",
    "\n",
    "\n",
    "\n",
    "Before training the model, you need to compile it. You will specify the loss function, optimizer, and evaluation metrics. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "945c8151",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T12:24:03.291994Z",
     "iopub.status.busy": "2024-12-11T12:24:03.291150Z",
     "iopub.status.idle": "2024-12-11T12:24:03.300397Z",
     "shell.execute_reply": "2024-12-11T12:24:03.299614Z"
    },
    "papermill": {
     "duration": 0.024087,
     "end_time": "2024-12-11T12:24:03.302080",
     "exception": false,
     "start_time": "2024-12-11T12:24:03.277993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffa737b",
   "metadata": {
    "papermill": {
     "duration": 0.012006,
     "end_time": "2024-12-11T12:24:03.326215",
     "exception": false,
     "start_time": "2024-12-11T12:24:03.314209",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In the above code: \n",
    "\n",
    "\n",
    "\n",
    "`optimizer='adam'` specifies the Adam optimizer, a popular choice for training neural networks. \n",
    "\n",
    "\n",
    "\n",
    "`loss='binary_crossentropy'` specifies the loss function for binary classification problems. \n",
    "\n",
    "\n",
    "\n",
    "`metrics=['accuracy']` tells Keras to evaluate the model using accuracy during training. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec5ec13",
   "metadata": {
    "papermill": {
     "duration": 0.011941,
     "end_time": "2024-12-11T12:24:03.350512",
     "exception": false,
     "start_time": "2024-12-11T12:24:03.338571",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Exercise 8: Train the Model \n",
    "\n",
    "\n",
    "\n",
    "You can now train the model on some training data. For this example, let's assume `X_train` is our training input data and `y_train` is the corresponding labels. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea9988aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T12:24:03.375878Z",
     "iopub.status.busy": "2024-12-11T12:24:03.375583Z",
     "iopub.status.idle": "2024-12-11T12:24:05.509059Z",
     "shell.execute_reply": "2024-12-11T12:24:05.508360Z"
    },
    "papermill": {
     "duration": 2.148252,
     "end_time": "2024-12-11T12:24:05.510714",
     "exception": false,
     "start_time": "2024-12-11T12:24:03.362462",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1733919844.069303      73 service.cc:145] XLA service 0x790a0c0072e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1733919844.069365      73 service.cc:153]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 1/32\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 1s/step - accuracy: 0.5000 - loss: 0.6878"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733919844.579740      73 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.4893 - loss: 0.6956\n",
      "Epoch 2/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5059 - loss: 0.6936 \n",
      "Epoch 3/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5249 - loss: 0.6904 \n",
      "Epoch 4/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5416 - loss: 0.6879 \n",
      "Epoch 5/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5346 - loss: 0.6839 \n",
      "Epoch 6/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5460 - loss: 0.6820 \n",
      "Epoch 7/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5662 - loss: 0.6800 \n",
      "Epoch 8/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5446 - loss: 0.6844 \n",
      "Epoch 9/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5474 - loss: 0.6810 \n",
      "Epoch 10/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5365 - loss: 0.6811 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x790ab77e1000>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Redefine the Model for 20 features\n",
    "\n",
    "model = Sequential([\n",
    "\n",
    "    Input(shape=(20,)),  # Adjust input shape to (20,)\n",
    "\n",
    "    Dense(128, activation='relu'),  # Hidden layer with 128 neurons and ReLU activation\n",
    "\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification with sigmoid activation\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Generate Example Data\n",
    "\n",
    "X_train = np.random.rand(1000, 20)  # 1000 samples, 20 features each\n",
    "\n",
    "y_train = np.random.randint(2, size=(1000, 1))  # 1000 binary labels (0 or 1)\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Train the Model\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3a68ca",
   "metadata": {
    "papermill": {
     "duration": 0.013719,
     "end_time": "2024-12-11T12:24:05.538608",
     "exception": false,
     "start_time": "2024-12-11T12:24:05.524889",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In the above code: \n",
    "\n",
    "\n",
    "\n",
    "`X_train` and `y_train` are placeholders for your actual training data. \n",
    "\n",
    "\n",
    "\n",
    "`model.fit` trains the model for a specified number of epochs and batch size. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2966df0c",
   "metadata": {
    "papermill": {
     "duration": 0.013433,
     "end_time": "2024-12-11T12:24:05.565615",
     "exception": false,
     "start_time": "2024-12-11T12:24:05.552182",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Exercise 9: Evaluate the Model \n",
    "\n",
    "\n",
    "\n",
    "After training, you can evaluate the model on test data to see how well it performs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3f2338d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T12:24:05.594046Z",
     "iopub.status.busy": "2024-12-11T12:24:05.593787Z",
     "iopub.status.idle": "2024-12-11T12:24:05.986093Z",
     "shell.execute_reply": "2024-12-11T12:24:05.985208Z"
    },
    "papermill": {
     "duration": 0.408685,
     "end_time": "2024-12-11T12:24:05.987760",
     "exception": false,
     "start_time": "2024-12-11T12:24:05.579075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.4916 - loss: 0.6961\n",
      "Test loss: 0.6996791362762451\n",
      "Test accuracy: 0.4749999940395355\n"
     ]
    }
   ],
   "source": [
    "# Example test data (in practice, use real dataset)\n",
    "\n",
    "X_test = np.random.rand(200, 20)  # 200 samples, 20 features each\n",
    "\n",
    "y_test = np.random.randint(2, size=(200, 1))  # 200 binary labels (0 or 1)\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "# Print test loss and accuracy\n",
    "\n",
    "print(f'Test loss: {loss}')\n",
    "\n",
    "print(f'Test accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49a6bec",
   "metadata": {
    "papermill": {
     "duration": 0.013625,
     "end_time": "2024-12-11T12:24:06.015658",
     "exception": false,
     "start_time": "2024-12-11T12:24:06.002033",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In the above code: \n",
    "\n",
    "\n",
    "\n",
    "`model.evaluate` computes the loss and accuracy of the model on test data. \n",
    "\n",
    "\n",
    "\n",
    "`X_test` and `y_test` are placeholders for your actual test data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc69339a",
   "metadata": {
    "papermill": {
     "duration": 0.013444,
     "end_time": "2024-12-11T12:24:06.042728",
     "exception": false,
     "start_time": "2024-12-11T12:24:06.029284",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Practice Exercises \n",
    "\n",
    "\n",
    "\n",
    "### Exercise 1: Basic Custom Training Loop \n",
    "\n",
    "\n",
    "\n",
    "#### Objective: Implement a basic custom training loop to train a simple neural network on the MNIST dataset. \n",
    "\n",
    "\n",
    "\n",
    "#### Instructions: \n",
    "\n",
    "\n",
    "\n",
    "- Set up the environment and load the dataset. \n",
    "\n",
    "\n",
    "\n",
    "- Define the model with a Flatten layer and two Dense layers. \n",
    "\n",
    "\n",
    "\n",
    "- Define the loss function and optimizer. \n",
    "\n",
    "\n",
    "\n",
    "- Implement a custom training loop to iterate over the dataset, compute the loss, and update the model's weights. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3c07a83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T12:24:06.070961Z",
     "iopub.status.busy": "2024-12-11T12:24:06.070683Z",
     "iopub.status.idle": "2024-12-11T12:27:30.032384Z",
     "shell.execute_reply": "2024-12-11T12:27:30.031449Z"
    },
    "papermill": {
     "duration": 203.991779,
     "end_time": "2024-12-11T12:27:30.048080",
     "exception": false,
     "start_time": "2024-12-11T12:24:06.056301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 0.03678794950246811\n",
      "Epoch 2: Loss = 0.03275846689939499\n",
      "Epoch 3: Loss = 0.03473801910877228\n",
      "Epoch 4: Loss = 0.03497496247291565\n",
      "Epoch 5: Loss = 0.02484220825135708\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "\n",
    "\n",
    "\n",
    "# Import necessary libraries\n",
    "\n",
    "import tensorflow as tf \n",
    "\n",
    "from tensorflow.keras.models import Sequential \n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten \n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Set Up the Environment\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() \n",
    "\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 \n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32) \n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Define the Model\n",
    "\n",
    "model = Sequential([ \n",
    "\n",
    "    Flatten(input_shape=(28, 28)), \n",
    "\n",
    "    Dense(128, activation='relu'), \n",
    "\n",
    "    Dense(10) \n",
    "\n",
    "]) \n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Define Loss Function and Optimizer\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) \n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam() \n",
    "\n",
    "\n",
    "\n",
    "# Step 4: Implement the Custom Training Loop\n",
    "\n",
    "for epoch in range(5): \n",
    "\n",
    "    for x_batch, y_batch in train_dataset: \n",
    "\n",
    "        with tf.GradientTape() as tape: \n",
    "\n",
    "            logits = model(x_batch, training=True) \n",
    "\n",
    "            loss = loss_fn(y_batch, logits) \n",
    "\n",
    "        grads = tape.gradient(loss, model.trainable_weights) \n",
    "\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights)) \n",
    "\n",
    "    print(f'Epoch {epoch + 1}: Loss = {loss.numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba78a1ad",
   "metadata": {
    "papermill": {
     "duration": 0.013811,
     "end_time": "2024-12-11T12:27:30.076565",
     "exception": false,
     "start_time": "2024-12-11T12:27:30.062754",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<details>\n",
    "\n",
    "<summary>Click here for solution</summary> </br>\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "# Import necessary libraries\n",
    "\n",
    "import tensorflow as tf \n",
    "\n",
    "from tensorflow.keras.models import Sequential \n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten \n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Set Up the Environment\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() \n",
    "\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 \n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32) \n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Define the Model\n",
    "\n",
    "model = Sequential([ \n",
    "\n",
    "    Flatten(input_shape=(28, 28)), \n",
    "\n",
    "    Dense(128, activation='relu'), \n",
    "\n",
    "    Dense(10) \n",
    "\n",
    "]) \n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Define Loss Function and Optimizer\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) \n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam() \n",
    "\n",
    "\n",
    "\n",
    "# Step 4: Implement the Custom Training Loop\n",
    "\n",
    "for epoch in range(5): \n",
    "\n",
    "    for x_batch, y_batch in train_dataset: \n",
    "\n",
    "        with tf.GradientTape() as tape: \n",
    "\n",
    "            logits = model(x_batch, training=True) \n",
    "\n",
    "            loss = loss_fn(y_batch, logits) \n",
    "\n",
    "        grads = tape.gradient(loss, model.trainable_weights) \n",
    "\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights)) \n",
    "\n",
    "    print(f'Epoch {epoch + 1}: Loss = {loss.numpy()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495606ad",
   "metadata": {
    "papermill": {
     "duration": 0.013801,
     "end_time": "2024-12-11T12:27:30.104378",
     "exception": false,
     "start_time": "2024-12-11T12:27:30.090577",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Exercise 2: Adding Accuracy Metric \n",
    "\n",
    "\n",
    "\n",
    "#### Objective: Enhance the custom training loop by adding an accuracy metric to monitor model performance. \n",
    "\n",
    "\n",
    "\n",
    "#### Instructions: \n",
    "\n",
    "\n",
    "\n",
    "1. Set up the environment and define the model, loss function, and optimizer. \n",
    "\n",
    "\n",
    "\n",
    "2. Add Sparse Categorical Accuracy as a metric. \n",
    "\n",
    "\n",
    "\n",
    "3. Implement the custom training loop with accuracy tracking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "339d788f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T12:27:30.133593Z",
     "iopub.status.busy": "2024-12-11T12:27:30.133271Z",
     "iopub.status.idle": "2024-12-11T12:31:08.721557Z",
     "shell.execute_reply": "2024-12-11T12:31:08.720409Z"
    },
    "papermill": {
     "duration": 218.62505,
     "end_time": "2024-12-11T12:31:08.743345",
     "exception": false,
     "start_time": "2024-12-11T12:27:30.118295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 0.034914396703243256 Accuracy = 0.9242666959762573\n",
      "Epoch 2: Loss = 0.025539489462971687 Accuracy = 0.9659666419029236\n",
      "Epoch 3: Loss = 0.0386647954583168 Accuracy = 0.9776833057403564\n",
      "Epoch 4: Loss = 0.029609568417072296 Accuracy = 0.9835500121116638\n",
      "Epoch 5: Loss = 0.011422747746109962 Accuracy = 0.9882833361625671\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "\n",
    "\n",
    "\n",
    "# Import necessary libraries\n",
    "\n",
    "import tensorflow as tf \n",
    "\n",
    "from tensorflow.keras.models import Sequential \n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten \n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Set Up the Environment\n",
    "\n",
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data() \n",
    "\n",
    "x_train = x_train / 255.0 \n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32) \n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Define the Model\n",
    "\n",
    "model = Sequential([ \n",
    "\n",
    "    Flatten(input_shape=(28, 28)), \n",
    "\n",
    "    Dense(128, activation='relu'), \n",
    "\n",
    "    Dense(10) \n",
    "\n",
    "]) \n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Define Loss Function, Optimizer, and Metric\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) \n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam() \n",
    "\n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy() \n",
    "\n",
    "\n",
    "\n",
    "# Step 4: Implement the Custom Training Loop with Accuracy Tracking\n",
    "\n",
    "epochs = 5 \n",
    "\n",
    "for epoch in range(epochs): \n",
    "\n",
    "    for x_batch, y_batch in train_dataset: \n",
    "\n",
    "        with tf.GradientTape() as tape: \n",
    "\n",
    "            logits = model(x_batch, training=True) \n",
    "\n",
    "            loss = loss_fn(y_batch, logits) \n",
    "\n",
    "        grads = tape.gradient(loss, model.trainable_weights) \n",
    "\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights)) \n",
    "\n",
    "        accuracy_metric.update_state(y_batch, logits) \n",
    "\n",
    "    print(f'Epoch {epoch + 1}: Loss = {loss.numpy()} Accuracy = {accuracy_metric.result().numpy()}') \n",
    "\n",
    "    accuracy_metric.reset_state() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2574c3cd",
   "metadata": {
    "papermill": {
     "duration": 0.056998,
     "end_time": "2024-12-11T12:31:08.824067",
     "exception": false,
     "start_time": "2024-12-11T12:31:08.767069",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<details>\n",
    "\n",
    "<summary>Click here for solution</summary><br>\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "# Import necessary libraries\n",
    "\n",
    "import tensorflow as tf \n",
    "\n",
    "from tensorflow.keras.models import Sequential \n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten \n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Set Up the Environment\n",
    "\n",
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data() \n",
    "\n",
    "x_train = x_train / 255.0 \n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32) \n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Define the Model\n",
    "\n",
    "model = Sequential([ \n",
    "\n",
    "    Flatten(input_shape=(28, 28)), \n",
    "\n",
    "    Dense(128, activation='relu'), \n",
    "\n",
    "    Dense(10) \n",
    "\n",
    "]) \n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Define Loss Function, Optimizer, and Metric\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) \n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam() \n",
    "\n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy() \n",
    "\n",
    "\n",
    "\n",
    "# Step 4: Implement the Custom Training Loop with Accuracy Tracking\n",
    "\n",
    "epochs = 5 \n",
    "\n",
    "for epoch in range(epochs): \n",
    "\n",
    "    for x_batch, y_batch in train_dataset: \n",
    "\n",
    "        with tf.GradientTape() as tape: \n",
    "\n",
    "            logits = model(x_batch, training=True) \n",
    "\n",
    "            loss = loss_fn(y_batch, logits) \n",
    "\n",
    "        grads = tape.gradient(loss, model.trainable_weights) \n",
    "\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights)) \n",
    "\n",
    "        accuracy_metric.update_state(y_batch, logits) \n",
    "\n",
    "    print(f'Epoch {epoch + 1}: Loss = {loss.numpy()} Accuracy = {accuracy_metric.result().numpy()}') \n",
    "\n",
    "    accuracy_metric.reset_state() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6006103",
   "metadata": {
    "papermill": {
     "duration": 0.014216,
     "end_time": "2024-12-11T12:31:08.852952",
     "exception": false,
     "start_time": "2024-12-11T12:31:08.838736",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Exercise 3: Custom Callback for Advanced Logging \n",
    "\n",
    "\n",
    "\n",
    "#### Objective: Implement a custom callback to log additional metrics and information during training. \n",
    "\n",
    "\n",
    "\n",
    "#### Instructions: \n",
    "\n",
    "\n",
    "\n",
    "1. Set up the environment and define the model, loss function, optimizer, and metric. \n",
    "\n",
    "\n",
    "\n",
    "2. Create a custom callback to log additional metrics at the end of each epoch. \n",
    "\n",
    "\n",
    "\n",
    "3. Implement the custom training loop with the custom callback. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11b8db69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T12:31:08.884380Z",
     "iopub.status.busy": "2024-12-11T12:31:08.883835Z",
     "iopub.status.idle": "2024-12-11T12:34:47.784247Z",
     "shell.execute_reply": "2024-12-11T12:34:47.783310Z"
    },
    "papermill": {
     "duration": 218.933251,
     "end_time": "2024-12-11T12:34:47.801131",
     "exception": false,
     "start_time": "2024-12-11T12:31:08.867880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 1, loss: 0.04763253405690193, accuracy: 0.921833336353302\n",
      "End of epoch 2, loss: 0.04389074072241783, accuracy: 0.9640499949455261\n",
      "End of epoch 3, loss: 0.02936832793056965, accuracy: 0.975600004196167\n",
      "End of epoch 4, loss: 0.04076539725065231, accuracy: 0.9822999835014343\n",
      "End of epoch 5, loss: 0.017438529059290886, accuracy: 0.987666666507721\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "\n",
    "\n",
    "\n",
    "# Import necessary libraries\n",
    "\n",
    "import tensorflow as tf \n",
    "\n",
    "from tensorflow.keras.models import Sequential \n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten \n",
    "\n",
    "from tensorflow.keras.callbacks import Callback \n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Set Up the Environment\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() \n",
    "\n",
    "x_train = x_train / 255.0 \n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32) \n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Define the Model\n",
    "\n",
    "model = Sequential([ \n",
    "\n",
    "    tf.keras.Input(shape=(28, 28)),  # Updated Input layer syntax\n",
    "\n",
    "    Flatten(), \n",
    "\n",
    "    Dense(128, activation='relu'), \n",
    "\n",
    "    Dense(10) \n",
    "\n",
    "]) \n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Define Loss Function, Optimizer, and Metric\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) \n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam() \n",
    "\n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy() \n",
    "\n",
    "\n",
    "\n",
    "# Step 4: Implement the Custom Callback\n",
    "\n",
    "class CustomCallback(Callback): \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None): \n",
    "\n",
    "        print(f'End of epoch {epoch + 1}, loss: {logs.get(\"loss\")}, accuracy: {logs.get(\"accuracy\")}') \n",
    "\n",
    "\n",
    "\n",
    "# Step 5: Implement the Custom Training Loop with Custom Callback\n",
    "\n",
    "custom_callback = CustomCallback() \n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(5): \n",
    "\n",
    "    for x_batch, y_batch in train_dataset: \n",
    "\n",
    "        with tf.GradientTape() as tape: \n",
    "\n",
    "            logits = model(x_batch, training=True) \n",
    "\n",
    "            loss = loss_fn(y_batch, logits) \n",
    "\n",
    "        grads = tape.gradient(loss, model.trainable_weights) \n",
    "\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights)) \n",
    "\n",
    "        accuracy_metric.update_state(y_batch, logits) \n",
    "\n",
    "    custom_callback.on_epoch_end(epoch, logs={'loss': loss.numpy(), 'accuracy': accuracy_metric.result().numpy()}) \n",
    "\n",
    "    accuracy_metric.reset_state()  # Updated method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb667312",
   "metadata": {
    "papermill": {
     "duration": 0.014269,
     "end_time": "2024-12-11T12:34:47.829832",
     "exception": false,
     "start_time": "2024-12-11T12:34:47.815563",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<details>\n",
    "\n",
    "<summary>Click here for solution</summary> </br>\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "# Import necessary libraries\n",
    "\n",
    "import tensorflow as tf \n",
    "\n",
    "from tensorflow.keras.models import Sequential \n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten \n",
    "\n",
    "from tensorflow.keras.callbacks import Callback \n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Set Up the Environment\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() \n",
    "\n",
    "x_train = x_train / 255.0 \n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32) \n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Define the Model\n",
    "\n",
    "model = Sequential([ \n",
    "\n",
    "    tf.keras.Input(shape=(28, 28)),  # Updated Input layer syntax\n",
    "\n",
    "    Flatten(), \n",
    "\n",
    "    Dense(128, activation='relu'), \n",
    "\n",
    "    Dense(10) \n",
    "\n",
    "]) \n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Define Loss Function, Optimizer, and Metric\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) \n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam() \n",
    "\n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy() \n",
    "\n",
    "\n",
    "\n",
    "# Step 4: Implement the Custom Callback\n",
    "\n",
    "class CustomCallback(Callback): \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None): \n",
    "\n",
    "        print(f'End of epoch {epoch + 1}, loss: {logs.get(\"loss\")}, accuracy: {logs.get(\"accuracy\")}') \n",
    "\n",
    "\n",
    "\n",
    "# Step 5: Implement the Custom Training Loop with Custom Callback\n",
    "\n",
    "custom_callback = CustomCallback() \n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(5): \n",
    "\n",
    "    for x_batch, y_batch in train_dataset: \n",
    "\n",
    "        with tf.GradientTape() as tape: \n",
    "\n",
    "            logits = model(x_batch, training=True) \n",
    "\n",
    "            loss = loss_fn(y_batch, logits) \n",
    "\n",
    "        grads = tape.gradient(loss, model.trainable_weights) \n",
    "\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights)) \n",
    "\n",
    "        accuracy_metric.update_state(y_batch, logits) \n",
    "\n",
    "    custom_callback.on_epoch_end(epoch, logs={'loss': loss.numpy(), 'accuracy': accuracy_metric.result().numpy()}) \n",
    "\n",
    "    accuracy_metric.reset_state()  # Updated method\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c804996",
   "metadata": {
    "papermill": {
     "duration": 0.014179,
     "end_time": "2024-12-11T12:34:47.858326",
     "exception": false,
     "start_time": "2024-12-11T12:34:47.844147",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Exercise 4: Lab - Hyperparameter Tuning \n",
    "\n",
    "\n",
    "\n",
    "#### Enhancement: Add functionality to save the results of each hyperparameter tuning iteration as JSON files in a specified directory. \n",
    "\n",
    "\n",
    "\n",
    "#### Additional Instructions:\n",
    "\n",
    "\n",
    "\n",
    "Modify the tuning loop to save each iteration's results as JSON files.\n",
    "\n",
    "\n",
    "\n",
    "Specify the directory where these JSON files will be stored for easier retrieval and analysis of tuning results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984f3127",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T12:34:47.888192Z",
     "iopub.status.busy": "2024-12-11T12:34:47.887909Z",
     "iopub.status.idle": "2024-12-11T12:35:11.519825Z",
     "shell.execute_reply": "2024-12-11T12:35:11.518675Z"
    },
    "papermill": {
     "duration": 23.648558,
     "end_time": "2024-12-11T12:35:11.521197",
     "exception": true,
     "start_time": "2024-12-11T12:34:47.872639",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from tuner_results\\hyperparam_tuning\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "import os\n",
    "\n",
    "import keras_tuner as kt\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Load your dataset\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Define the model-building function\n",
    "\n",
    "def build_model(hp):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Tune the number of units in the first Dense layer\n",
    "\n",
    "    model.add(Dense(units=hp.Int('units', min_value=32, max_value=512, step=32),\n",
    "\n",
    "                    activation='relu'))\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Binary classification example\n",
    "\n",
    "    model.compile(optimizer=Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='LOG')),\n",
    "\n",
    "                  loss='binary_crossentropy',\n",
    "\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Initialize a Keras Tuner RandomSearch tuner\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "\n",
    "    build_model,\n",
    "\n",
    "    objective='val_accuracy',\n",
    "\n",
    "    max_trials=10,  # Set the number of trials\n",
    "\n",
    "    executions_per_trial=1,  # Set how many executions per trial\n",
    "\n",
    "    directory='tuner_results',  # Directory for saving logs\n",
    "\n",
    "    project_name='hyperparam_tuning'\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Step 4: Run the tuner search (make sure the data is correct)\n",
    "\n",
    "tuner.search(X_train, y_train, validation_data=(X_val, y_val), epochs=5)\n",
    "\n",
    "\n",
    "\n",
    "# Step 5: Save the tuning results as JSON files\n",
    "\n",
    "# delete the previous directory if it exists\n",
    "import shutil\n",
    "if os.path.exists('tuning_results'):\n",
    "    shutil.rmtree('tuning_results')\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs('tuning_results', exist_ok=True) \n",
    "\n",
    "\n",
    "try:\n",
    "\n",
    "    for i in range(10):\n",
    "\n",
    "        # Fetch the best hyperparameters from the tuner\n",
    "\n",
    "        best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "        \n",
    "\n",
    "        # Results dictionary to save hyperparameters and score\n",
    "\n",
    "        results = {\n",
    "\n",
    "            \"trial\": i + 1,\n",
    "\n",
    "            \"hyperparameters\": best_hps.values,  # Hyperparameters tuned in this trial\n",
    "\n",
    "            \"score\": None  # Add any score or metrics if available\n",
    "\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "        # Save the results as JSON\n",
    "\n",
    "        with open(os.path.join('tuning_results', f\"trial_{i + 1}.json\"), \"w\") as f:\n",
    "\n",
    "            json.dump(results, f)\n",
    "\n",
    "\n",
    "\n",
    "except IndexError:\n",
    "\n",
    "    print(\"Tuning process has not completed or no results available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ebc2156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 02s]\n",
      "val_accuracy: 0.800000011920929\n",
      "\n",
      "Best val_accuracy So Far: 0.8849999904632568\n",
      "Total elapsed time: 00h 00m 21s\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Step 1: Load your dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Step 2: Define the model-building function\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    # Tune the number of units in the first Dense layer\n",
    "    model.add(Dense(units=hp.Int('units', min_value=32, max_value=512, step=32),\n",
    "                    activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Binary classification example\n",
    "    model.compile(optimizer=Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='LOG')),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Step 3: Initialize a Keras Tuner BayesianOptimization tuner\n",
    "tuner = kt.BayesianOptimization(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,  # Set the number of trials\n",
    "    executions_per_trial=1,  # Set how many executions per trial\n",
    "    directory='tuner_results',  # Directory for saving logs\n",
    "    project_name='hyperparam_tuning'\n",
    ")\n",
    "\n",
    "# Step 4: Run the tuner search (make sure the data is correct)\n",
    "tuner.search(X_train, y_train, validation_data=(X_val, y_val), epochs=5)\n",
    "\n",
    "# Step 5: Save the tuning results as JSON files\n",
    "# delete the previous directory if it exists\n",
    "import shutil\n",
    "if os.path.exists('tuning_results'):\n",
    "    shutil.rmtree('tuning_results')\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs('tuning_results', exist_ok=True) \n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    for i in range(10):\n",
    "        # Fetch the best hyperparameters from the tuner\n",
    "        best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "        # Results dictionary to save hyperparameters and score\n",
    "        results = {\n",
    "            \"trial\": i + 1,\n",
    "            \"hyperparameters\": best_hps.values,  # Hyperparameters tuned in this trial\n",
    "            \"score\": None  # Add any score or metrics if available\n",
    "        }\n",
    "\n",
    "        # Save the results as JSON\n",
    "        with open(os.path.join('tuning_results', f\"trial_{i + 1}.json\"), \"w\") as f:\n",
    "            json.dump(results, f)\n",
    "\n",
    "except IndexError:\n",
    "    print(\"Tuning process has not completed or no results available.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dadcbe",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<details>\n",
    "\n",
    "<summary>Click here for solution</summary> </br>\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "!pip install keras-tuner\n",
    "\n",
    "# !pip install scikit-learn\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "import os\n",
    "\n",
    "import keras_tuner as kt\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Load your dataset\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Define the model-building function\n",
    "\n",
    "def build_model(hp):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Tune the number of units in the first Dense layer\n",
    "\n",
    "    model.add(Dense(units=hp.Int('units', min_value=32, max_value=512, step=32),\n",
    "\n",
    "                    activation='relu'))\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Binary classification example\n",
    "\n",
    "    model.compile(optimizer=Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='LOG')),\n",
    "\n",
    "                  loss='binary_crossentropy',\n",
    "\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Initialize a Keras Tuner RandomSearch tuner\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "\n",
    "    build_model,\n",
    "\n",
    "    objective='val_accuracy',\n",
    "\n",
    "    max_trials=10,  # Set the number of trials\n",
    "\n",
    "    executions_per_trial=1,  # Set how many executions per trial\n",
    "\n",
    "    directory='tuner_results',  # Directory for saving logs\n",
    "\n",
    "    project_name='hyperparam_tuning'\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Step 4: Run the tuner search (make sure the data is correct)\n",
    "\n",
    "tuner.search(X_train, y_train, validation_data=(X_val, y_val), epochs=5)\n",
    "\n",
    "\n",
    "\n",
    "# Step 5: Save the tuning results as JSON files\n",
    "\n",
    "try:\n",
    "\n",
    "    for i in range(10):\n",
    "\n",
    "        # Fetch the best hyperparameters from the tuner\n",
    "\n",
    "        best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "        \n",
    "\n",
    "        # Results dictionary to save hyperparameters and score\n",
    "\n",
    "        results = {\n",
    "\n",
    "            \"trial\": i + 1,\n",
    "\n",
    "            \"hyperparameters\": best_hps.values,  # Hyperparameters tuned in this trial\n",
    "\n",
    "            \"score\": None  # Add any score or metrics if available\n",
    "\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "        # Save the results as JSON\n",
    "\n",
    "        with open(os.path.join('tuning_results', f\"trial_{i + 1}.json\"), \"w\") as f:\n",
    "\n",
    "            json.dump(results, f)\n",
    "\n",
    "\n",
    "\n",
    "except IndexError:\n",
    "\n",
    "    print(\"Tuning process has not completed or no results available.\")\n",
    "\n",
    " ```   \n",
    "\n",
    "\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50472896",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Exercise 5: Explanation of Hyperparameter Tuning\n",
    "\n",
    "\n",
    "\n",
    "**Addition to Explanation:** Add a note explaining the purpose of num_trials in the hyperparameter tuning context:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ed1412",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Write your code here\n",
    "\n",
    "\n",
    "\n",
    "Explanation: \"num_trials specifies the number of top hyperparameter sets to return. Setting num_trials=1 means that it will return only the best set of hyperparameters found during the tuning process.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1c7c37",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<details>\n",
    "\n",
    "<summary>Click here for solution</summary> </br>\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "Explanation: \"num_trials specifies the number of top hyperparameter sets to return. Setting num_trials=1 means that it will return only the best set of hyperparameters found during the tuning process.\"\n",
    "\n",
    " ```   \n",
    "\n",
    "\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4101ae8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Conclusion: \n",
    "\n",
    "\n",
    "\n",
    "Congratulations on completing this lab! You have now successfully created, trained, and evaluated a simple neural network model using the Keras Functional API. This foundational knowledge will allow you to build more complex models and explore advanced functionalities in Keras. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e479a1be",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Copyright © IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1074.002127,
   "end_time": "2024-12-11T12:35:14.389944",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-11T12:17:20.387817",
   "version": "2.6.0"
  },
  "prev_pub_hash": "48a1eb2565c8b635156cd21708473ccadb84e292e93f3530a9d5223b7590344e"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
